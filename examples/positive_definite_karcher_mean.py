# Autogenerated with SMOP 
from smop.core import *
# positive_definite_karcher_mean.m

    
@function
def positive_definite_karcher_mean(A=None,*args,**kwargs):
    varargin = positive_definite_karcher_mean.varargin
    nargin = positive_definite_karcher_mean.nargin

    # Computes a Karcher mean of a collection of positive definite matrices.
    
    # function X = positive_definite_karcher_mean(A)
    
    # Input:  A 3D matrix A of size nxnxm such that each slice A(:,:,k) is a
#         positive definite matrix of size nxn.
# 
# Output: A positive definite matrix X of size nxn which is a Karcher mean
#         of the m matrices in A, that is, X minimizes the sum of squared
#         Riemannian distances to the matrices in A:
#            f(X) = sum_k=1^m .5*dist^2(X, A(:, :, k))
#         The distance is defined by the natural metric on the set of
#         positive definite matrices: dist(X,Y) = norm(logm(X\Y), 'fro').
# 
# This simple example is not the best way to compute Karcher means. Its
# purpose it to serve as base code to explore other algorithms. In
# particular, in the presence of large noise, this algorithm seems to not
# be able to reach points with a very small gradient norm. This may be
# caused by insufficient accuracy in the gradient computation.
    
    # This file is part of Manopt and is copyrighted. See the license file.
# 
# Main author: Nicolas Boumal, Sept. 3, 2013
# Contributors:
# 
# Change log:
#
    
    # Generate some random data to test the function if none is given.
    if logical_not(exist('A','var')) or isempty(A):
        n=5
# positive_definite_karcher_mean.m:32
        m=10
# positive_definite_karcher_mean.m:33
        A=zeros(n,n,m)
# positive_definite_karcher_mean.m:34
        ref=diag(max(0.1,1 + dot(0.1,randn(n,1))))
# positive_definite_karcher_mean.m:35
        for i in arange(1,m).reshape(-1):
            noise=dot(0.01,randn(n))
# positive_definite_karcher_mean.m:37
            noise=(noise + noise.T) / 2
# positive_definite_karcher_mean.m:38
            V,D=eig(ref + noise,nargout=2)
# positive_definite_karcher_mean.m:39
            A[:,:,i]=dot(dot(V,diag(max(0.01,diag(D)))),V.T)
# positive_definite_karcher_mean.m:40
    
    
    # Retrieve the size of the problem:
    # There are m matrices of size nxn to average.
    n=size(A,1)
# positive_definite_karcher_mean.m:46
    m=size(A,3)
# positive_definite_karcher_mean.m:47
    assert_(n == size(A,2),cat('The slices of A must be square, i.e., the ','first and second dimensions of A must be equal.'))
    
    # Notice that this is the only place we specify on which manifold we
    # wish to compute Karcher means. Replacing this factory for another
    # geometry will yield code to compute Karcher means on that other
    # manifold, provided that manifold is equipped with a dist function and
    # a logarithmic map log.
    M=sympositivedefinitefactory(n)
# positive_definite_karcher_mean.m:58
    
    # function and its gradient.
    problem.M = copy(M)
# positive_definite_karcher_mean.m:62
    problem.cost = copy(cost)
# positive_definite_karcher_mean.m:63
    problem.grad = copy(grad)
# positive_definite_karcher_mean.m:64
    
    problem.approxhess = copy(approxhessianFD(problem,struct('stepsize',0.0001)))
# positive_definite_karcher_mean.m:67
    
    # performance hit can be alleviated by using the caching system. We go
    # for a simple implementation here, as a tutorial example.
    
    # Cost function
    
@function
def cost(X=None,*args,**kwargs):
    varargin = cost.varargin
    nargin = cost.nargin

    f=0
# positive_definite_karcher_mean.m:75
    for k in arange(1,m).reshape(-1):
        f=f + M.dist(X,A[:,:,k]) ** 2
# positive_definite_karcher_mean.m:77
    
    f=f / (dot(2,m))
# positive_definite_karcher_mean.m:79
    return f
    
if __name__ == '__main__':
    pass
    
    # Riemannian gradient of the cost function
    
@function
def grad(X=None,*args,**kwargs):
    varargin = grad.varargin
    nargin = grad.nargin

    g=M.zerovec(X)
# positive_definite_karcher_mean.m:84
    for k in arange(1,m).reshape(-1):
        # Update g in a linear combination of the form
            # g = g - [something]/m.
        g=M.lincomb(X,1,g,- 1 / m,M.log(X,A[:,:,k]))
# positive_definite_karcher_mean.m:88
    
    return g
    
if __name__ == '__main__':
    pass
    
    
    # Execute some checks on the derivatives for early debugging.
    # These things can be commented out of course.
    # The slopes should agree on part of the plot at least. In this case,
    # it is sometimes necessary to inspect the plot visually to make the
    # call, but it is indeed correct.
    # checkgradient(problem);
    # pause;
    
    # Execute this if you want to force using a proper parallel vector
    # transport. This is not necessary. If you omit this, the default
    # vector transport is the identity map, which is (of course) cheaper
    # and seems to perform well in practice.
    # M.transp = M.paralleltransp;
    
    # Issue a call to a solver. Default options are selected.
    # Our initial guess is the first data point.
    X=trustregions(problem,A[:,:,1])
# positive_definite_karcher_mean.m:108
    return g
    
if __name__ == '__main__':
    pass
    