# Autogenerated with SMOP 
from smop.core import *
# low_rank_tensor_completion.m

    
@function
def low_rank_tensor_completion(*args,**kwargs):
    varargin = low_rank_tensor_completion.varargin
    nargin = low_rank_tensor_completion.nargin

    # Given partial observation of a low rank tensor, attempts to complete it.
    
    # function low_rank_tensor_completion()
    
    # This example demonstrates how to use the geometry factory for the
# quotient manifold of fixed-rank tensors, 
# fixedrankfactory_tucker_preconditioned.
    
    # This geometry is described in the technical report
# "Riemannian preconditioning for tensor completion"
# Hiroyuki Kasai and Bamdev Mishra, arXiv:1506.02159, 2015.
    
    # This can be a starting point for many optimization problems of the form:
    
    # minimize f(X) such that rank(X) = [r1 r2 r3], size(X) = [n1, n2, n3].
    
    # Input:  None. This example file generates random data.
# 
# Output: None.
    
    # Please cite the Manopt paper as well as the research paper:
#     @Techreport{kasai2015,
#       Title   = {{R}iemannian preconditioning for tensor completion},
#       Author  = {Kasai, H. and Mishra, B.},
#       Journal = {Arxiv preprint arXiv:1506.02159},
#       Year    = {2015}
#     }
    
    # This file is part of Manopt and is copyrighted. See the license file.
# 
# Main authors: Hiroyuki Kasai and Bamdev Mishra, June 16, 2015.
# Contributors:
# 
# Change log:
#
    
    # Random data generation with pseudo-random numbers from a 
    # uniform distribution on [0, 1].
    # First, choose the size of the problem.
    # We will complete a tensor of size n1-by-n2-by-n3 of rank (r1, r2, r3):
    n1=70
# low_rank_tensor_completion.m:43
    n2=60
# low_rank_tensor_completion.m:44
    n3=50
# low_rank_tensor_completion.m:45
    r1=3
# low_rank_tensor_completion.m:46
    r2=4
# low_rank_tensor_completion.m:47
    r3=5
# low_rank_tensor_completion.m:48
    tensor_dims=matlabarray(cat(n1,n2,n3))
# low_rank_tensor_completion.m:49
    core_dims=matlabarray(cat(r1,r2,r3))
# low_rank_tensor_completion.m:50
    total_entries=dot(dot(n1,n2),n3)
# low_rank_tensor_completion.m:51
    
    U1,R1=qr(rand(n1,r1),0,nargout=2)
# low_rank_tensor_completion.m:54
    U2,R2=qr(rand(n2,r2),0,nargout=2)
# low_rank_tensor_completion.m:55
    U3,R3=qr(rand(n3,r3),0,nargout=2)
# low_rank_tensor_completion.m:56
    Z.U1 = copy(R1)
# low_rank_tensor_completion.m:58
    Z.U2 = copy(R2)
# low_rank_tensor_completion.m:59
    Z.U3 = copy(R3)
# low_rank_tensor_completion.m:60
    Z.G = copy(rand(core_dims))
# low_rank_tensor_completion.m:61
    Core=tucker2multiarray(Z)
# low_rank_tensor_completion.m:62
    
    Y.U1 = copy(U1)
# low_rank_tensor_completion.m:64
    Y.U2 = copy(U2)
# low_rank_tensor_completion.m:65
    Y.U3 = copy(U3)
# low_rank_tensor_completion.m:66
    Y.G = copy(Core)
# low_rank_tensor_completion.m:67
    A=tucker2multiarray(Y)
# low_rank_tensor_completion.m:68
    
    # (i, j, k) of A is observed, and 0 otherwise.    
    # Observation ratio
    fraction=0.1
# low_rank_tensor_completion.m:73
    
    nr=round(dot(fraction,total_entries))
# low_rank_tensor_completion.m:74
    ind=randperm(total_entries)
# low_rank_tensor_completion.m:75
    ind=ind[1:nr]
# low_rank_tensor_completion.m:76
    P=false(tensor_dims)
# low_rank_tensor_completion.m:77
    P[ind]=true
# low_rank_tensor_completion.m:78
    
    PA=multiply(P,A)
# low_rank_tensor_completion.m:80
    
    problem.M = copy(fixedrankfactory_tucker_preconditioned(tensor_dims,core_dims))
# low_rank_tensor_completion.m:86
    
    # fields U1, U2, U3, G representing a rank (r1,r2,r3) tensor.
    # f(X) = 1/2 * || P.*(X - A) ||^2
    problem.cost = copy(cost)
# low_rank_tensor_completion.m:94
    
@function
def cost(X=None,*args,**kwargs):
    varargin = cost.varargin
    nargin = cost.nargin

    Xmultiarray=tucker2multiarray(X)
# low_rank_tensor_completion.m:96
    Diffmultiarray=multiply(P,Xmultiarray) - PA
# low_rank_tensor_completion.m:97
    Diffmultiarray_flat=reshape(Diffmultiarray,n1,dot(n2,n3))
# low_rank_tensor_completion.m:98
    f=dot(0.5,norm(Diffmultiarray_flat,'fro') ** 2)
# low_rank_tensor_completion.m:99
    return f
    
if __name__ == '__main__':
    pass
    
    
    
    # Define the Euclidean gradient of the cost function, that is, the
    # gradient of f(X) seen as a standard function of X.
    # nabla f(X) = P.*(X-A)
    # We only need to give the Euclidean gradient. Manopt converts it
    # internally to the Riemannian counterpart.
    problem.egrad = copy(egrad)
# low_rank_tensor_completion.m:110
    
@function
def egrad(X=None,*args,**kwargs):
    varargin = egrad.varargin
    nargin = egrad.nargin

    Xmultiarray=tucker2multiarray(X)
# low_rank_tensor_completion.m:112
    Smultiarray=multiply(P,Xmultiarray) - PA
# low_rank_tensor_completion.m:113
    
    S1multiarray=reshape(Smultiarray,cat(n1,dot(n2,n3)))
# low_rank_tensor_completion.m:116
    S2multiarray=reshape(permute(Smultiarray,cat(2,1,3)),cat(n2,dot(n1,n3)))
# low_rank_tensor_completion.m:117
    S3multiarray=reshape(permute(Smultiarray,cat(3,1,2)),cat(n3,dot(n1,n2)))
# low_rank_tensor_completion.m:118
    g.U1 = copy(dot(dot(double(S1multiarray),kron(X.U3,X.U2)),reshape(X.G,r1,dot(r2,r3)).T))
# low_rank_tensor_completion.m:120
    g.U2 = copy(dot(dot(double(S2multiarray),kron(X.U3,X.U1)),reshape(permute(X.G,cat(2,1,3)),r2,dot(r1,r3)).T))
# low_rank_tensor_completion.m:121
    g.U3 = copy(dot(dot(double(S3multiarray),kron(X.U2,X.U1)),reshape(permute(X.G,cat(3,1,2)),r3,dot(r1,r2)).T))
# low_rank_tensor_completion.m:122
    g.G = copy(reshape(dot(dot(X.U1.T,reshape(double(Smultiarray),n1,dot(n2,n3))),kron(X.U3.T,X.U2.T).T),r1,r2,r3))
# low_rank_tensor_completion.m:123
    return g
    
if __name__ == '__main__':
    pass
    
    
    
    
    
    
    # Define the Euclidean Hessian of the cost at X, along eta, where eta is
    # represented as a tangent vector: a structure with fields U1, U2, U3, G.
    # This is the directional derivative of nabla f(X) at X along Xdot:
    # nabla^2 f(X)[Xdot] = P.*Xdot
    # We only need to give the Euclidean Hessian. Manopt converts it
    # internally to the Riemannian counterpart.
    problem.ehess = copy(ehess)
# low_rank_tensor_completion.m:136
    
@function
def ehess(X=None,eta=None,*args,**kwargs):
    varargin = ehess.varargin
    nargin = ehess.nargin

    # Computing S, and its unfolding matrices, S1, S2, and S3.
    Xmultiarray=tucker2multiarray(X)
# low_rank_tensor_completion.m:140
    S=multiply(P,Xmultiarray) - PA
# low_rank_tensor_completion.m:141
    S1=reshape(S,cat(n1,dot(n2,n3)))
# low_rank_tensor_completion.m:142
    S2=reshape(permute(S,cat(2,1,3)),cat(n2,dot(n1,n3)))
# low_rank_tensor_completion.m:143
    S3=reshape(permute(S,cat(3,1,2)),cat(n3,dot(n1,n2)))
# low_rank_tensor_completion.m:144
    
    XG=X.G
# low_rank_tensor_completion.m:147
    etaG=eta.G
# low_rank_tensor_completion.m:148
    G1=zeros(dot(4,size(X.G)))
# low_rank_tensor_completion.m:149
    G1[1:r1,1:r2,1:r3]=XG
# low_rank_tensor_completion.m:150
    G1[r1 + 1:dot(2,r1),r2 + 1:dot(2,r2),r3 + 1:dot(2,r3)]=XG
# low_rank_tensor_completion.m:151
    G1[dot(2,r1) + 1:dot(3,r1),dot(2,r2) + 1:dot(3,r2),dot(2,r3) + 1:dot(3,r3)]=XG
# low_rank_tensor_completion.m:152
    G1[dot(3,r1) + 1:dot(4,r1),dot(3,r2) + 1:dot(4,r2),dot(3,r3) + 1:dot(4,r3)]=etaG
# low_rank_tensor_completion.m:153
    X1.G = copy(G1)
# low_rank_tensor_completion.m:155
    X1.U1 = copy(cat(eta.U1,X.U1,X.U1,X.U1))
# low_rank_tensor_completion.m:156
    X1.U2 = copy(cat(X.U2,eta.U2,X.U2,X.U2))
# low_rank_tensor_completion.m:157
    X1.U3 = copy(cat(X.U3,X.U3,eta.U3,X.U3))
# low_rank_tensor_completion.m:158
    X1multiarray=tucker2multiarray(X1)
# low_rank_tensor_completion.m:160
    Sdot=multiply(P,X1multiarray)
# low_rank_tensor_completion.m:161
    S1dot=reshape(Sdot,cat(n1,dot(n2,n3)))
# low_rank_tensor_completion.m:162
    S2dot=reshape(permute(Sdot,cat(2,1,3)),cat(n2,dot(n1,n3)))
# low_rank_tensor_completion.m:163
    S3dot=reshape(permute(Sdot,cat(3,1,2)),cat(n3,dot(n1,n2)))
# low_rank_tensor_completion.m:164
    
    X_G1=reshape(double(X.G),r1,dot(r2,r3))
# low_rank_tensor_completion.m:167
    X_G2=reshape(permute(double(X.G),cat(2,1,3)),r2,dot(r1,r3))
# low_rank_tensor_completion.m:168
    X_G3=reshape(permute(double(X.G),cat(3,1,2)),r3,dot(r1,r2))
# low_rank_tensor_completion.m:169
    eta_G1=reshape(double(eta.G),r1,dot(r2,r3))
# low_rank_tensor_completion.m:170
    eta_G2=reshape(permute(double(eta.G),cat(2,1,3)),r2,dot(r1,r3))
# low_rank_tensor_completion.m:171
    eta_G3=reshape(permute(double(eta.G),cat(3,1,2)),r3,dot(r1,r2))
# low_rank_tensor_completion.m:172
    
    T1=dot(double(S1dot),(dot(kron(X.U3,X.U2),X_G1.T))) + dot(double(S1),(dot(kron(eta.U3,X.U2),X_G1.T) + dot(kron(X.U3,eta.U2),X_G1.T) + dot(kron(X.U3,X.U2),eta_G1.T)))
# low_rank_tensor_completion.m:175
    T2=dot(double(S2dot),(dot(kron(X.U3,X.U1),X_G2.T))) + dot(double(S2),(dot(kron(eta.U3,X.U1),X_G2.T) + dot(kron(X.U3,eta.U1),X_G2.T) + dot(kron(X.U3,X.U1),eta_G2.T)))
# low_rank_tensor_completion.m:179
    T3=dot(double(S3dot),(dot(kron(X.U2,X.U1),X_G3.T))) + dot(double(S3),(dot(kron(eta.U2,X.U1),X_G3.T) + dot(kron(X.U2,eta.U1),X_G3.T) + dot(kron(X.U2,X.U1),eta_G3.T)))
# low_rank_tensor_completion.m:183
    Hess.U1 = copy(T1)
# low_rank_tensor_completion.m:187
    Hess.U2 = copy(T2)
# low_rank_tensor_completion.m:188
    Hess.U3 = copy(T3)
# low_rank_tensor_completion.m:189
    
    N.U1 = copy(X.U1.T)
# low_rank_tensor_completion.m:192
    N.U2 = copy(X.U2.T)
# low_rank_tensor_completion.m:193
    N.U3 = copy(X.U3.T)
# low_rank_tensor_completion.m:194
    N.G = copy(Sdot)
# low_rank_tensor_completion.m:195
    M0array=tucker2multiarray(N)
# low_rank_tensor_completion.m:196
    M1.U1 = copy(eta.U1.T)
# low_rank_tensor_completion.m:198
    M1.U2 = copy(X.U2.T)
# low_rank_tensor_completion.m:199
    M1.U3 = copy(X.U3.T)
# low_rank_tensor_completion.m:200
    M1.G = copy(S)
# low_rank_tensor_completion.m:201
    M1array=tucker2multiarray(M1)
# low_rank_tensor_completion.m:202
    M2.U1 = copy(X.U1.T)
# low_rank_tensor_completion.m:204
    M2.U2 = copy(eta.U2.T)
# low_rank_tensor_completion.m:205
    M2.U3 = copy(X.U3.T)
# low_rank_tensor_completion.m:206
    M2.G = copy(S)
# low_rank_tensor_completion.m:207
    M2array=tucker2multiarray(M2)
# low_rank_tensor_completion.m:208
    M3.U1 = copy(X.U1.T)
# low_rank_tensor_completion.m:210
    M3.U2 = copy(X.U2.T)
# low_rank_tensor_completion.m:211
    M3.U3 = copy(eta.U3.T)
# low_rank_tensor_completion.m:212
    M3.G = copy(S)
# low_rank_tensor_completion.m:213
    M3array=tucker2multiarray(M3)
# low_rank_tensor_completion.m:214
    Hess.G = copy(M0array + M1array + M2array + M3array)
# low_rank_tensor_completion.m:216
    return Hess
    
if __name__ == '__main__':
    pass
    
    
    
    # Check consistency of the gradient and the Hessian. Useful if you
    # adapt this example for a new cost function and you would like to make
    # sure there is no mistake.
    
    # Notice that the checkhessian test fails: the slope is not right. 
    # This is because the retraction is not second-order compatible with 
    # the Riemannian exponential on this manifold, making 
    # the checkhessian tool unusable. The Hessian is correct though. 
    # # warning('off', 'manopt:fixedrankfactory_tucker_preconditioned:exp');
    # # checkgradient(problem);
    # # drawnow;
    # # pause;
    # # checkhessian(problem);
    # # drawnow;
    # # pause;
    
    
    # options
    options.maxiter = copy(200)
# low_rank_tensor_completion.m:241
    options.maxinner = copy(30)
# low_rank_tensor_completion.m:242
    options.maxtime = copy(inf)
# low_rank_tensor_completion.m:243
    options.tolgradnorm = copy(1e-05)
# low_rank_tensor_completion.m:244
    
    Xtr=trustregions(problem,[],options)
# low_rank_tensor_completion.m:250
    
    # U1, U2, U3 and G.
    Xtrmultiarray=tucker2multiarray(Xtr)
# low_rank_tensor_completion.m:256
    fprintf('||X-A||_F = %g\\n',norm(reshape(Xtrmultiarray - A,cat(n1,dot(n2,n3))),'fro'))
    
    # or conjugategradient (CG). These solvers need to solve a
    # line-search problem at each iteration. Standard line searches in
    # Manopt have generic purpose systems to do this. But for the problem
    # at hand, we could exploit the least-squares structure to compute an
    # approximate stepsize for the line-search problem. The approximation
    # is obtained by linearizing the nonlinear manifold locally and further
    # approximating it with a degree 2 polynomial approximation.
    # The specific derivation is in the paper referenced above.
    
    problem.linesearch = copy(linesearch_helper)
# low_rank_tensor_completion.m:272
    
@function
def linesearch_helper(X=None,eta=None,*args,**kwargs):
    varargin = linesearch_helper.varargin
    nargin = linesearch_helper.nargin

    
    # term0
    Xmultiarray=tucker2multiarray(X)
# low_rank_tensor_completion.m:276
    residual_mat=multiply(P,Xmultiarray) - PA
# low_rank_tensor_completion.m:277
    residual_vec=ravel(residual_mat)
# low_rank_tensor_completion.m:278
    term0=copy(residual_vec)
# low_rank_tensor_completion.m:279
    
    XG=X.G
# low_rank_tensor_completion.m:282
    etaG=eta.G
# low_rank_tensor_completion.m:283
    G1=zeros(dot(4,size(X.G)))
# low_rank_tensor_completion.m:284
    G1[1:r1,1:r2,1:r3]=XG
# low_rank_tensor_completion.m:285
    G1[r1 + 1:dot(2,r1),r2 + 1:dot(2,r2),r3 + 1:dot(2,r3)]=XG
# low_rank_tensor_completion.m:286
    G1[dot(2,r1) + 1:dot(3,r1),dot(2,r2) + 1:dot(3,r2),dot(2,r3) + 1:dot(3,r3)]=XG
# low_rank_tensor_completion.m:287
    G1[dot(3,r1) + 1:dot(4,r1),dot(3,r2) + 1:dot(4,r2),dot(3,r3) + 1:dot(4,r3)]=etaG
# low_rank_tensor_completion.m:288
    X1.U1 = copy(cat(eta.U1,X.U1,X.U1,X.U1))
# low_rank_tensor_completion.m:290
    X1.U2 = copy(cat(X.U2,eta.U2,X.U2,X.U2))
# low_rank_tensor_completion.m:291
    X1.U3 = copy(cat(X.U3,X.U3,eta.U3,X.U3))
# low_rank_tensor_completion.m:292
    X1.G = copy(G1)
# low_rank_tensor_completion.m:293
    X1multiarray=tucker2multiarray(X1)
# low_rank_tensor_completion.m:295
    term1_mat=multiply(P,X1multiarray)
# low_rank_tensor_completion.m:296
    term1=ravel(term1_mat)
# low_rank_tensor_completion.m:297
    
    # the coefficients a1 and a2 are shown below.
    a2=(dot(term1.T,term1))
# low_rank_tensor_completion.m:301
    a1=dot(2,(dot(term1.T,term0)))
# low_rank_tensor_completion.m:302
    tmin=dot(- 0.5,(a1 / a2))
# low_rank_tensor_completion.m:303
    return tmin
    
if __name__ == '__main__':
    pass
    
    # Notice that for this solver, the Hessian is not needed.
    Xcg,costcg,infocg=conjugategradient(problem,[],options,nargout=3)
# low_rank_tensor_completion.m:308
    fprintf('Take a look at the options that CG used:\\n')
    disp(options)
    fprintf('And see how many trials were made at each line search call:\\n')
    info_ls=matlabarray(cat(infocg.linesearch))
# low_rank_tensor_completion.m:313
    disp(cat(info_ls.costevals))
    fprintf('Try it again without the linesearch helper.\\n')
    
    problem=rmfield(problem,'linesearch')
# low_rank_tensor_completion.m:321
    Xcg,xcost,info,options=conjugategradient(problem,[],nargout=4)
# low_rank_tensor_completion.m:323
    
    
    fprintf('Take a look at the options that CG used:\\n')
    disp(options)
    fprintf('And see how many trials were made at each line search call:\\n')
    info_ls=matlabarray(cat(info.linesearch))
# low_rank_tensor_completion.m:328
    disp(cat(info_ls.costevals))
    return tmin
    
if __name__ == '__main__':
    pass
    