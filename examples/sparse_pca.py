# Autogenerated with SMOP 
from smop.core import *
# sparse_pca.m

    
@function
def sparse_pca(A=None,m=None,gamma=None,*args,**kwargs):
    varargin = sparse_pca.varargin
    nargin = sparse_pca.nargin

    # Sparse principal component analysis based on optimization over Stiefel.
    
    # [Z, P, X] = sparse_pca(A, m, gamma)
    
    # We consider sparse PCA applied to a data matrix A of size pxn, where p is
# the number of samples (observations) and n is the number of variables
# (features). We attempt to extract m different components. The parameter
# gamma, which must lie between 0 and the largest 2-norm of a column of
# A, tunes the balance between best explanation of the variance of the data
# (gamma = 0, mostly corresponds to standard PCA) and best sparsity of the
# principal components Z (gamma maximal, Z is zero). The variables
# contained in the columns of A are assumed centered (zero-mean).
    
    # The output Z of size nxm represents the principal components. There are m
# columns, each one of unit norm and capturing a prefered direction of the
# data, while trying to be sparse. P has the same size as Z and represents
# the sparsity pattern of Z. X is an orthonormal matrix of size pxm
# produced internally by the algorithm.
    
    # With classical PCA, the variability captured by m components is
# sum(svds(A, m))
# With the outputted Z, which should be sparser than normal PCA, it is
# sum(svd(A*Z))
    
    # The method is based on the maximization of a differentiable function over
# the Stiefel manifold of dimension pxm. Notice that this dimension is
# independent of n, making this method particularly suitable for problems
# with many variables but few samples (n much larger than p). The
# complexity of each iteration of the algorithm is linear in n as a result.
    
    # The theory behind this code is available in the paper
# http://jmlr.org/papers/volume11/journee10a/journee10a.pdf
# Generalized Power Method for Sparse Principal Component Analysis, by
# Journee, Nesterov, Richtarik and Sepulchre, JMLR, 2010.
# This implementation is not equivalent to the one described in that paper
# (and is independent from their authors) but is close in spirit
# nonetheless. It is provided with Manopt as an example file but was not
# optimized: please do not judge the quality of the algorithm described by
# the authors of the paper based on this implementation.
    
    # This file is part of Manopt and is copyrighted. See the license file.
# 
# Main author: Nicolas Boumal, Dec. 24, 2013
# Contributors:
# 
# Change log:
#
    
    # If no input is provided, generate random data for a quick demo
    if nargin == 0:
        n=100
# sparse_pca.m:52
        p=10
# sparse_pca.m:53
        m=2
# sparse_pca.m:54
        A=randn(p,n)
# sparse_pca.m:57
        # 2-norm of a column of A.
        gamma=1
# sparse_pca.m:61
    else:
        if nargin != 3:
            error('Please provide 3 inputs (or none for a demo).')
    
    
    # Execute the main algorithm: it will compute a sparsity pattern P.
    P,X=sparse_pca_stiefel_l1(A,m,gamma,nargout=2)
# sparse_pca.m:68
    
    Z=postprocess(A,P,X)
# sparse_pca.m:71
    return Z,P,X,A
    
if __name__ == '__main__':
    pass
    
    # Sparse PCA based on the block sparse PCA algorithm with l1-penalty as
# featured in the reference paper by Journee et al. This is not the same
# algorithm but it is the same cost function optimized over the same search
# space. We force N = eye(m).
    
@function
def sparse_pca_stiefel_l1(A=None,m=None,gamma=None,*args,**kwargs):
    varargin = sparse_pca_stiefel_l1.varargin
    nargin = sparse_pca_stiefel_l1.nargin

    
    p,n=size(A,nargout=2)
# sparse_pca.m:82
    
    # The optimization takes place over a Stiefel manifold whose dimension
    # is independent of n. This is especially useful when there are many
    # more variables than samples.
    St=stiefelfactory(p,m)
# sparse_pca.m:87
    problem.M = copy(St)
# sparse_pca.m:88
    
    # whether the caching structure 'store' has been populated with
    # quantities that are useful to compute at X or not. If they were not,
    # then we compute and store them now.
    
@function
def prepare(X=None,store=None,*args,**kwargs):
    varargin = prepare.varargin
    nargin = prepare.nargin

    if logical_not(isfield(store,'ready')) or logical_not(store.ready):
        store.AtX = copy(dot(A.T,X))
# sparse_pca.m:96
        store.absAtX = copy(abs(store.AtX))
# sparse_pca.m:97
        store.pos = copy(max(0,store.absAtX - gamma))
# sparse_pca.m:98
        store.ready = copy(true)
# sparse_pca.m:99
    
    return store
    
if __name__ == '__main__':
    pass
    
    # Define the cost function here and set it in the problem structure.
    problem.cost = copy(cost)
# sparse_pca.m:104
    
@function
def cost(X=None,store=None,*args,**kwargs):
    varargin = cost.varargin
    nargin = cost.nargin

    store=prepare(X,store)
# sparse_pca.m:106
    pos=store.pos
# sparse_pca.m:107
    f=dot(- 0.5,norm(pos,'fro') ** 2)
# sparse_pca.m:108
    return f,store
    
if __name__ == '__main__':
    pass
    
    # Here, we chose to define the Euclidean gradient (egrad instead of
    # grad) : Manopt will take care of converting it to the Riemannian
    # gradient.
    problem.egrad = copy(egrad)
# sparse_pca.m:114
    
@function
def egrad(X=None,store=None,*args,**kwargs):
    varargin = egrad.varargin
    nargin = egrad.nargin

    if logical_not(isfield(store,'G')):
        store=prepare(X,store)
# sparse_pca.m:117
        pos=store.pos
# sparse_pca.m:118
        AtX=store.AtX
# sparse_pca.m:119
        sgAtX=sign(AtX)
# sparse_pca.m:120
        factor=multiply(pos,sgAtX)
# sparse_pca.m:121
        store.G = copy(dot(- A,factor))
# sparse_pca.m:122
    
    G=store.G
# sparse_pca.m:124
    return G,store
    
if __name__ == '__main__':
    pass
    
    # checkgradient(problem);
    # pause;
    
    # The optimization happens here. To improve the method, it may be
    # interesting to investigate better-than-random initial iterates and,
    # possibly, to fine tune the parameters of the solver.
    X=trustregions(problem)
# sparse_pca.m:133
    
    P=abs(dot(A.T,X)) > gamma
# sparse_pca.m:136
    return G,store
    
if __name__ == '__main__':
    pass
    
    # This post-processing algorithm produces a matrix Z of size nxm matching
# the sparsity pattern P and representing sparse principal components for
# A. This is to be called with the output of the main algorithm. This
# algorithm is described in the reference paper by Journee et al.
    
@function
def postprocess(A=None,P=None,X=None,*args,**kwargs):
    varargin = postprocess.varargin
    nargin = postprocess.nargin

    fprintf('Post-processing... ')
    counter=0
# sparse_pca.m:147
    maxiter=1000
# sparse_pca.m:148
    tolerance=1e-08
# sparse_pca.m:149
    while counter < maxiter:

        Z=dot(A.T,X)
# sparse_pca.m:151
        Z[logical_not(P)]=0
# sparse_pca.m:152
        Z=dot(Z,diag(1.0 / sqrt(diag(dot(Z.T,Z)))))
# sparse_pca.m:153
        X=ufactor(dot(A,Z))
# sparse_pca.m:154
        counter=counter + 1
# sparse_pca.m:155
        if counter > 1 and norm(Z0 - Z,'fro') < dot(tolerance,norm(Z0,'fro')):
            break
        Z0=copy(Z)
# sparse_pca.m:159

    
    fprintf('done, in %d iterations (max = %d).\\n',counter,maxiter)
    return Z
    
if __name__ == '__main__':
    pass
    
    # Returns the U-factor of the polar decomposition of X
    
@function
def ufactor(X=None,*args,**kwargs):
    varargin = ufactor.varargin
    nargin = ufactor.nargin

    W,S,V=svd(X,0,nargout=3)
# sparse_pca.m:166
    
    U=dot(W,V.T)
# sparse_pca.m:167
    return U
    
if __name__ == '__main__':
    pass
    