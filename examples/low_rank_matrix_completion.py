# Autogenerated with SMOP 
from smop.core import *
# low_rank_matrix_completion.m

    
@function
def low_rank_matrix_completion(*args,**kwargs):
    varargin = low_rank_matrix_completion.varargin
    nargin = low_rank_matrix_completion.nargin

    # Given partial observation of a low rank matrix, attempts to complete it.
    
    # function low_rank_matrix_completion()
    
    # This example demonstrates how to use the geometry factory for the
# embedded submanifold of fixed-rank matrices, fixedrankembeddedfactory.
# This geometry is described in the paper
# "Low-rank matrix completion by Riemannian optimization"
# Bart Vandereycken - SIAM Journal on Optimization, 2013.
    
    # This can be a starting point for many optimization problems of the form:
    
    # minimize f(X) such that rank(X) = k, size(X) = [m, n].
    
    # Note that the code is long because it showcases quite a few features of
# Manopt: most of the code is optional.
    
    # Input:  None. This example file generates random data.
# 
# Output: None.
    
    # This file is part of Manopt and is copyrighted. See the license file.
# 
# Main author: Nicolas Boumal, July 15, 2014
# Contributors: Bart Vandereycken
# 
# Change log:
#
    
    # Random data generation. First, choose the size of the problem.
    # We will complete a matrix of size mxn of rank k:
    m=200
# low_rank_matrix_completion.m:33
    n=500
# low_rank_matrix_completion.m:34
    k=10
# low_rank_matrix_completion.m:35
    
    L=randn(m,k)
# low_rank_matrix_completion.m:37
    R=randn(n,k)
# low_rank_matrix_completion.m:38
    A=dot(L,R.T)
# low_rank_matrix_completion.m:39
    
    # (i, j) of A is observed, and 0 otherwise.
    fraction=dot(dot(4,k),(m + n - k)) / (dot(m,n))
# low_rank_matrix_completion.m:42
    P=sparse(rand(m,n) <= fraction)
# low_rank_matrix_completion.m:43
    
    PA=multiply(P,A)
# low_rank_matrix_completion.m:45
    
    problem.M = copy(fixedrankembeddedfactory(m,n,k))
# low_rank_matrix_completion.m:54
    
    # fields U, S, V representing a rank k matrix as U*S*V'.
    # f(X) = 1/2 * || P.*(X-A) ||^2
    problem.cost = copy(cost)
# low_rank_matrix_completion.m:59
    
@function
def cost(X=None,*args,**kwargs):
    varargin = cost.varargin
    nargin = cost.nargin

    # Note that it is very much inefficient to explicitly construct the
        # matrix X in this way. Seen as we only need to know the entries
        # of Xmat corresponding to the mask P, it would be far more
        # efficient to compute those only.
    Xmat=dot(dot(X.U,X.S),X.V.T)
# low_rank_matrix_completion.m:65
    f=dot(0.5,norm(multiply(P,Xmat) - PA,'fro') ** 2)
# low_rank_matrix_completion.m:66
    return f
    
if __name__ == '__main__':
    pass
    
    # Define the Euclidean gradient of the cost function, that is, the
    # gradient of f(X) seen as a standard function of X.
    # nabla f(X) = P.*(X-A)
    problem.egrad = copy(egrad)
# low_rank_matrix_completion.m:72
    
@function
def egrad(X=None,*args,**kwargs):
    varargin = egrad.varargin
    nargin = egrad.nargin

    # Same comment here about Xmat.
    Xmat=dot(dot(X.U,X.S),X.V.T)
# low_rank_matrix_completion.m:75
    G=multiply(P,Xmat) - PA
# low_rank_matrix_completion.m:76
    return G
    
if __name__ == '__main__':
    pass
    
    # This is optional, but it's nice if you have it.
    # Define the Euclidean Hessian of the cost at X, along H, where H is
    # represented as a tangent vector: a structure with fields Up, Vp, M.
    # This is the directional derivative of nabla f(X) at X along Xdot:
    # nabla^2 f(X)[Xdot] = P.*Xdot
    problem.ehess = copy(euclidean_hessian)
# low_rank_matrix_completion.m:84
    
@function
def euclidean_hessian(X=None,H=None,*args,**kwargs):
    varargin = euclidean_hessian.varargin
    nargin = euclidean_hessian.nargin

    # The function tangent2ambient transforms H (a tangent vector) into
        # its equivalent ambient vector representation. The output is a
        # structure with fields U, S, V such that U*S*V' is an mxn matrix
        # corresponding to the tangent vector H. Note that there are no
        # additional guarantees about U, S and V. In particular, U and V
        # are not orthonormal.
    ambient_H=problem.M.tangent2ambient(X,H)
# low_rank_matrix_completion.m:92
    Xdot=dot(dot(ambient_H.U,ambient_H.S),ambient_H.V.T)
# low_rank_matrix_completion.m:93
    
    # vector as an mxn matrix Xdot: we only need its entries
        # corresponding to the mask P, and this could be computed
        # efficiently.
    ehess=multiply(P,Xdot)
# low_rank_matrix_completion.m:98
    return ehess
    
if __name__ == '__main__':
    pass
    
    
    # Check consistency of the gradient and the Hessian. Useful if you
    # adapt this example for a new cost function and you would like to make
    # sure there is no mistake.
    # warning('off', 'manopt:fixedrankembeddedfactory:exp');
    # checkgradient(problem); pause;
    # checkhessian(problem); pause;
    
    
    
    
    
    # Compute an initial guess. Points on the manifold are represented as
    # structures with three fields: U, S and V. U and V need to be
    # orthonormal, S needs to be diagonal.
    U,S,V=svds(PA,k,nargout=3)
# low_rank_matrix_completion.m:118
    X0.U = copy(U)
# low_rank_matrix_completion.m:119
    X0.S = copy(S)
# low_rank_matrix_completion.m:120
    X0.V = copy(V)
# low_rank_matrix_completion.m:121
    
    # from the initial guess X0.
    X=trustregions(problem,X0)
# low_rank_matrix_completion.m:125
    
    # U, S and V.
    Xmat=dot(dot(X.U,X.S),X.V.T)
# low_rank_matrix_completion.m:129
    fprintf('||X-A||_F = %g\\n',norm(Xmat - A,'fro'))
    
    # steepestdescent or conjugategradient. These solvers need to solve a
    # line-search problem at each iteration. Standard line searches in
    # Manopt have generic purpose systems to do this. But for the problem
    # at hand, it so happens that we can rather accurately guess how far
    # the line-search should look, and it would be a waste to not use that.
    # Look up the paper referenced above for the mathematical explanation
    # of the code below.
    # 
    # To tell Manopt about this special information, we specify the
    # linesearch hint function in the problem structure. Notice that this
    # is not the same thing as specifying a linesearch function in the
    # options structure.
    # 
    # Both the SD and the CG solvers will detect that we
    # specify the hint function below, and they will use an appropriate
    # linesearch algorithm by default, as a result. Typically, they will
    # try the step t*H first, then if it does not satisfy an Armijo
    # criterion, they will decrease t geometrically until satisfaction or
    # failure.
    # 
    # Just like the cost, egrad and ehess functions, the linesearch
    # function could use a store structure if you like. The present code
    # does not use the store structure, which means quite a bit of the
    # computations are made redundantly, and as a result a better method
    # could appear slower. See the Manopt tutorial about caching when you
    # are ready to switch from a proof-of-concept code to an efficient
    # code.
    
    # The inputs are X (a point on the manifold) and H, a tangent vector at
    # X that is assumed to be a descent direction. That is, there exists a
    # positive t such that f(Retraction_X(tH)) < f(X). The function below
    # is supposed to output a "t" that it is a good "guess" at such a t.
    problem.linesearch = copy(linesearch_helper)
# low_rank_matrix_completion.m:170
    
@function
def linesearch_helper(X=None,H=None,*args,**kwargs):
    varargin = linesearch_helper.varargin
    nargin = linesearch_helper.nargin

    # Note that you would not usually need the Hessian for this.
    residual_omega=nonzeros(problem.egrad(X))
# low_rank_matrix_completion.m:173
    dir_omega=nonzeros(problem.ehess(X,H))
# low_rank_matrix_completion.m:174
    t=numpy.linalg.solve(- dir_omega,residual_omega)
# low_rank_matrix_completion.m:175
    return t
    
if __name__ == '__main__':
    pass
    
    # Notice that for this solver, the Hessian is not needed.
    Xcg,xcost,info,options=conjugategradient(problem,X0,nargout=4)
# low_rank_matrix_completion.m:179
    
    
    fprintf('Take a look at the options that CG used:\\n')
    disp(options)
    fprintf('And see how many trials were made at each line search call:\\n')
    info_ls=matlabarray(cat(info.linesearch))
# low_rank_matrix_completion.m:184
    disp(cat(info_ls.costevals))
    fprintf('Try it again without the linesearch helper.\\n')
    
    problem=rmfield(problem,'linesearch')
# low_rank_matrix_completion.m:197
    Xcg,xcost,info,options=conjugategradient(problem,X0,nargout=4)
# low_rank_matrix_completion.m:199
    
    
    fprintf('Take a look at the options that CG used:\\n')
    disp(options)
    fprintf('And see how many trials were made at each line search call:\\n')
    info_ls=matlabarray(cat(info.linesearch))
# low_rank_matrix_completion.m:204
    disp(cat(info_ls.costevals))
    
    # purposes) compute the spectrum of the Hessian at a point X. This may
    # help in studying the conditioning of a problem. If you don't provide
    # the Hessian, Manopt will approximate the Hessian with finite
    # differences of the gradient and try to estimate its "spectrum" (it's
    # not a proper linear operator). This can give some intuition, but
    # should not be relied upon.
    if problem.M.dim() < 100:
        fprintf('Computing the spectrum of the Hessian...')
        s=hessianspectrum(problem,X)
# low_rank_matrix_completion.m:222
        hist(s)
    
    
    
    
    
    return t
    
if __name__ == '__main__':
    pass
    