# Autogenerated with SMOP 
from smop.core import *
# thomson_problem.m

    
@function
def thomson_problem(n=None,d=None,*args,**kwargs):
    varargin = thomson_problem.varargin
    nargin = thomson_problem.nargin

    # Simple attempt at computing n well distributed points on a sphere in R^d.
# 
# This is an example of how Manopt can approximate the gradient and even
# the Hessian of a cost function based on finite differences, even if only
# the cost function is specified without its derivatives.
    
    # This functionality is provided only as a help for prototyping, and should
# not be used to compare algorithms in terms of computation time or
# accuracy, because the underlying gradient approximation scheme is slow.
    
    # See also the derivative free solvers for an alternative:
# pso and neldermead.
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, Nov. 1, 2016
# Contributors:
# Change log:
    
    if logical_not(exist('n','var')) or isempty(n):
        n=50
# thomson_problem.m:21
    
    if logical_not(exist('d','var')) or isempty(d):
        d=3
# thomson_problem.m:24
    
    # Define the Thomson problem with 1/r^2 potential. That is: find n points
# x_i on a sphere in R^d such that the sum over all pairs (i, j) of the
# potentials 1/||x_i - x_j||^2 is minimized. Since the points are on a
# sphere, each potential is .5/(1-x_i'*x_j).
    problem.M = copy(obliquefactory(d,n))
# thomson_problem.m:31
    problem.cost = copy(lambda X=None: sum(sum(triu(1.0 / (1 - dot(X.T,X)),1))) / n ** 2)
# thomson_problem.m:32
    # Attempt to minimize the cost. Since the gradient is not provided, Manopt
# approximates it with finite differences. This is /slow/, since for each
# gradient approximation, problem.M.dim()+1 calls to the cost function are
# necessary, on top of generating an orthonormal basis of the tangent space
# at each iterate.
    
    # Note that it is difficult to reach high accuracy critical points with an
# approximate gradient, hence the need to set a less ambitious value for
# the gradient norm tolerance.
    opts.tolgradnorm = copy(0.001)
# thomson_problem.m:43
    X=conjugategradient(problem,[],opts)
# thomson_problem.m:44
    # Plot the points on a translucid sphere
    if nargout == 0 and d == 3:
        x,y,z=sphere(50,nargout=3)
# thomson_problem.m:48
        surf(x,y,z,'FaceAlpha',0.5)
        hold('all')
        plot3(X[1,:],X[2,:],X[3,:],'.','MarkerSize',20)
        axis('equal')
        box('off')
        axis('off')
    
    # For much better performance, after an early prototyping phase, the
# gradient of the cost function should be specified, typically in
# problem.grad or problem.egrad. See the online document at
    
    #   http://www.manopt.org
    
    # for more information.
    
    return X
    
if __name__ == '__main__':
    pass
    