# Autogenerated with SMOP 
from smop.core import *
# truncated_svd.m

    
@function
def truncated_svd(A=None,p=None,*args,**kwargs):
    varargin = truncated_svd.varargin
    nargin = truncated_svd.nargin

    # Returns an SVD decomposition of A truncated to rank p.
    
    # function [U, S, V, info] = truncated_svd(A, p)
    
    # Input: A real matrix A of size mxn and an integer p <= min(m, n).
# Output: An orthonormal matrix U of size mxp, an orthonormal matrix Y of
#         size nxp and a diagonal matrix S of size pxp with nonnegative and
#         decreasing diagonal entries such that USV.' is the best rank p
#         approximation of A according to the Frobenius norm. All real.
#         This function produces an output akin to svds.
# 
# The decomposition is obtained by maximizing
#   f(U, V) = .5*norm(U'*A*V, 'fro')^2
# where U, V are orthonormal. Notice that f(U*Q, V*R) = f(U, V) for all
# Q, R orthogonal pxp matrices. Hence, only the column spaces of U and V
# matter and we may perform the optimization over a product of two
# Grassmannian manifolds.
    
    # It is easy to show that maximizing f is equivalent to minimizing g with
#   g(U, V) = min_S norm(U*S*V' - A, 'fro')^2,
# which confirms that we are going for a best low-rank approximation of A.
# 
# The inner workings of the Grassmann manifold use the built-in svd
# function of Matlab but only for matrices of size mxp and nxp to
# re-orthonormalize them.
# 
# Notice that we are actually chasing a best fixed-rank approximation of a
# matrix, which is best obtained by working directly over a manifold of
# fixed-rank matrices. This is simply an example script to demonstrate some
# functionalities of the toolbox.
# 
# The code can be modified to accept a function handle for A(x) = A*x
# instead of a matrix A, which is often useful. This would further require
# a function handle At for the transpose of A, such that At(x) = A.'*x.
    
    # This file is part of Manopt and is copyrighted. See the license file.
# 
# Main author: Nicolas Boumal, July 5, 2013
# Contributors:
# 
# Change log:
#
    
    
    # Generate some random data to test the function if none is given.
    if logical_not(exist('A','var')) or isempty(A):
        A=randn(42,60)
# truncated_svd.m:48
    
    if logical_not(exist('p','var')) or isempty(p):
        p=5
# truncated_svd.m:51
    
    
    # Retrieve the size of the problem and make sure the requested
    # approximation rank is at most the maximum possible rank.
    m,n=size(A,nargout=2)
# truncated_svd.m:56
    assert_(p <= min(m,n),'p must be smaller than the smallest dimension of A.')
    
    tuple.U = copy(grassmannfactory(m,p))
# truncated_svd.m:60
    tuple.V = copy(grassmannfactory(n,p))
# truncated_svd.m:61
    
    # property of the cost function indicated above and thus place U and V
    # on the Stiefel manifold (orthonormal matrices) instead of the
    # Grassmann manifold. Working on Stiefel is expected to be slower
    # though, partly because de search space is higher dimensional and
    # partly because the optimizers are not isolated.
    # tuple.U = stiefelfactory(m, p);
    # tuple.V = stiefelfactory(n, p);
    M=productmanifold(tuple)
# truncated_svd.m:70
    
    # function and its derivatives. Here, to demonstrate the rapid
    # prototyping capabilities of Manopt, we directly define the Euclidean
    # gradient and the Euclidean Hessian egrad and ehess instead of the
    # Riemannian gradient and Hessian grad and hess. Manopt will take care
    # of the conversion. This automatic conversion is usually not
    # computationally optimal though, because much of the computations
    # involved in obtaining the gradient could be reused to obtain the
    # Hessian. After the prototyping stage, when efficiency becomes
    # important, it makes sense to define grad and hess rather than egrad
    # an ehess, and to use the caching system (the store structure).
    problem.M = copy(M)
# truncated_svd.m:83
    problem.cost = copy(cost)
# truncated_svd.m:84
    problem.egrad = copy(egrad)
# truncated_svd.m:85
    problem.ehess = copy(ehess)
# truncated_svd.m:86
    
    # performance hit can be alleviated by using the caching system.
    
    # Cost function
    
@function
def cost(X=None,*args,**kwargs):
    varargin = cost.varargin
    nargin = cost.nargin

    U=X.U
# truncated_svd.m:93
    V=X.V
# truncated_svd.m:94
    f=dot(- 0.5,norm(dot(dot(U.T,A),V),'fro') ** 2)
# truncated_svd.m:95
    return f
    
if __name__ == '__main__':
    pass
    
    # Euclidean gradient of the cost function
    
@function
def egrad(X=None,*args,**kwargs):
    varargin = egrad.varargin
    nargin = egrad.nargin

    U=X.U
# truncated_svd.m:99
    V=X.V
# truncated_svd.m:100
    AV=dot(A,V)
# truncated_svd.m:101
    AtU=dot(A.T,U)
# truncated_svd.m:102
    g.U = copy(dot(- AV,(dot(AV.T,U))))
# truncated_svd.m:103
    g.V = copy(dot(- AtU,(dot(AtU.T,V))))
# truncated_svd.m:104
    return g
    
if __name__ == '__main__':
    pass
    
    # Euclidean Hessian of the cost function
    
@function
def ehess(X=None,H=None,*args,**kwargs):
    varargin = ehess.varargin
    nargin = ehess.nargin

    U=X.U
# truncated_svd.m:108
    V=X.V
# truncated_svd.m:109
    Udot=H.U
# truncated_svd.m:110
    Vdot=H.V
# truncated_svd.m:111
    AV=dot(A,V)
# truncated_svd.m:112
    AtU=dot(A.T,U)
# truncated_svd.m:113
    AVdot=dot(A,Vdot)
# truncated_svd.m:114
    AtUdot=dot(A.T,Udot)
# truncated_svd.m:115
    h.U = copy(- (dot(dot(AVdot,AV.T),U) + dot(dot(AV,AVdot.T),U) + dot(dot(AV,AV.T),Udot)))
# truncated_svd.m:116
    h.V = copy(- (dot(dot(AtUdot,AtU.T),V) + dot(dot(AtU,AtUdot.T),V) + dot(dot(AtU,AtU.T),Vdot)))
# truncated_svd.m:117
    return h
    
if __name__ == '__main__':
    pass
    
    
    
    # Execute some checks on the derivatives for early debugging.
    # These things can be commented out of course.
    # checkgradient(problem);
    # pause;
    # checkhessian(problem);
    # pause;
    
    # Issue a call to a solver. A random initial guess will be chosen and
    # default options are selected. Here, we specify a maximum trust
    # region radius (which in turn induces an initial trust region radius).
    # Note that this is not required: default values are used if we omit
    # this. The diameter of the manifold scales like sqrt(2*p), hence the
    # form of our (empirical) choice.
    options.Delta_bar = copy(dot(4,sqrt(dot(2,p))))
# truncated_svd.m:134
    X,Xcost,info=trustregions(problem,[],options,nargout=3)
# truncated_svd.m:135
    
    U=X.U
# truncated_svd.m:136
    V=X.V
# truncated_svd.m:137
    
    # be diagonal with nonnegative, decreasing entries. This requires a
    # small svd of size pxp.
    Spp=dot(dot(U.T,A),V)
# truncated_svd.m:142
    Upp,Spp,Vpp=svd(Spp,nargout=3)
# truncated_svd.m:143
    U=dot(U,Upp)
# truncated_svd.m:144
    S=copy(Spp)
# truncated_svd.m:145
    V=dot(V,Vpp)
# truncated_svd.m:146
    
    # Riemannian Hessian on the tangent space at (any) X. Computing the
    # spectrum at the solution gives us some idea of the conditioning of
    # the problem. If we were to implement a preconditioner for the
    # Hessian, this would also inform us on its performance.
    
    # Notice that if the optimization is performed on a product of Stiefel
    # manifolds instead of a product of Grassmannians, the double
    # invariance under the orthogonal group O(p) will appear as twice
    # p*(p-1)/2, thus p*(p-1) zero eigenvalues in the spectrum of the
    # Hessian. This means that the minimizers are not isolated, which
    # typically hinders convergence of second order algorithms.
    if M.dim() < 512:
        evs=hessianspectrum(problem,X)
# truncated_svd.m:161
        stairs(sort(evs))
        title(cat('Eigenvalues of the Hessian of the cost function ','at the solution'))
    
    return h
    
if __name__ == '__main__':
    pass
    