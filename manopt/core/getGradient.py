# Autogenerated with SMOP 
from smop.core import *
# getGradient.m

    
@function
def getGradient(problem=None,x=None,storedb=None,key=None,*args,**kwargs):
    varargin = getGradient.varargin
    nargin = getGradient.nargin

    # Computes the gradient of the cost function at x.
    
    # function grad = getGradient(problem, x)
# function grad = getGradient(problem, x, storedb)
# function grad = getGradient(problem, x, storedb, key)
    
    # Returns the gradient at x of the cost function described in the problem
# structure.
    
    # storedb is a StoreDB object, key is the StoreDB key to point x.
    
    # See also: getDirectionalDerivative canGetGradient
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, Dec. 30, 2012.
# Contributors: 
# Change log:
    
    #   April 3, 2015 (NB):
#       Works with the new StoreDB class system.
    
    #  June 28, 2016 (NB):
#       Works with getPartialGradient.
    
    #   Nov. 1, 2016 (NB):
#       Added support for gradient from directional derivatives.
#       Last resort is call to getApproxGradient instead of an exception.
    
    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# getGradient.m:33
        key=storedb.getNewKey()
# getGradient.m:35
    
    
    if isfield(problem,'grad'):
        ## Compute the gradient using grad.
        # Check whether this function wants to deal with storedb or not.
        if 1 == nargin(problem.grad):
            grad=problem.grad(x)
# getGradient.m:45
        else:
            if 2 == nargin(problem.grad):
                # Obtain, pass along, and save the store for x.
                store=storedb.getWithShared(key)
# getGradient.m:48
                grad,store=problem.grad(x,store,nargout=2)
# getGradient.m:49
                storedb.setWithShared(store,key)
            else:
                if 3 == nargin(problem.grad):
                    # Pass along the whole storedb (by reference), with key.
                    grad=problem.grad(x,storedb,key)
# getGradient.m:53
                else:
                    up=MException('manopt:getGradient:badgrad','grad should accept 1, 2 or 3 inputs.')
# getGradient.m:55
                    throw(up)
    else:
        if isfield(problem,'costgrad'):
            ## Compute the gradient using costgrad.
            # Check whether this function wants to deal with storedb or not.
            if 1 == nargin(problem.costgrad):
                unused,grad=problem.costgrad(x,nargout=2)
# getGradient.m:66
            else:
                if 2 == nargin(problem.costgrad):
                    # Obtain, pass along, and save the store for x.
                    store=storedb.getWithShared(key)
# getGradient.m:69
                    unused,grad,store=problem.costgrad(x,store,nargout=3)
# getGradient.m:70
                    storedb.setWithShared(store,key)
                else:
                    if 3 == nargin(problem.costgrad):
                        # Pass along the whole storedb (by reference), with key.
                        unused,grad=problem.costgrad(x,storedb,key,nargout=2)
# getGradient.m:74
                    else:
                        up=MException('manopt:getGradient:badcostgrad','costgrad should accept 1, 2 or 3 inputs.')
# getGradient.m:76
                        throw(up)
        else:
            if canGetEuclideanGradient(problem):
                ## Compute the gradient using the Euclidean gradient.
                egrad=getEuclideanGradient(problem,x,storedb,key)
# getGradient.m:84
                grad=problem.M.egrad2rgrad(x,egrad)
# getGradient.m:85
            else:
                if canGetPartialGradient(problem):
                    ## Compute the gradient using a full partial gradient.
                    d=problem.ncostterms
# getGradient.m:90
                    grad=getPartialGradient(problem,x,arange(1,d),storedb,key)
# getGradient.m:91
                else:
                    if canGetDirectionalDerivative(problem):
                        ## Compute gradient based on directional derivatives; expensive!
                        B=tangentorthobasis(problem.M,x)
# getGradient.m:96
                        df=zeros(size(B))
# getGradient.m:97
                        for k in arange(1,numel(B)).reshape(-1):
                            df[k]=getDirectionalDerivative(problem,x,B[k],storedb,key)
# getGradient.m:99
                        grad=lincomb(problem.M,x,B,df)
# getGradient.m:101
                    else:
                        ## Attempt the computation of an approximation of the gradient.
                        grad=getApproxGradient(problem,x,storedb,key)
# getGradient.m:106
    
    
    return grad
    
if __name__ == '__main__':
    pass
    