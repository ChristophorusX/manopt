# Autogenerated with SMOP 
from smop.core import *
# getPartialGradient.m

    
@function
def getPartialGradient(problem=None,x=None,I=None,storedb=None,key=None,*args,**kwargs):
    varargin = getPartialGradient.varargin
    nargin = getPartialGradient.nargin

    # Computes the gradient of a subset of terms in the cost function at x.
    
    # function grad = getPartialGradient(problem, x, I)
# function grad = getPartialGradient(problem, x, I, storedb)
# function grad = getPartialGradient(problem, x, I, storedb, key)
    
    # Assume the cost function described in the problem structure is a sum of
# many terms, as
    
    #    f(x) = sum_i f_i(x) for i = 1:d,
    
    # where d is specified as d = problem.ncostterms.
# 
# For a subset I of 1:d, getPartialGradient obtains the gradient of the
# partial cost function
# 
#    f_I(x) = sum_i f_i(x) for i = I.
    
    # storedb is a StoreDB object, key is the StoreDB key to point x.
    
    # See also: getGradient canGetPartialGradient getPartialEuclideanGradient
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, June 28, 2016
# Contributors: 
# Change log:
    
    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# getPartialGradient.m:33
        key=storedb.getNewKey()
# getPartialGradient.m:35
    
    
    
    # Make sure I is a row vector, so that it is natural to loop over it
    # with " for i = I ".
    I=(ravel(I)).T
# getPartialGradient.m:41
    if isfield(problem,'partialgrad'):
        ## Compute the partial gradient using partialgrad.
        # Check whether this function wants to deal with storedb or not.
        if 2 == nargin(problem.partialgrad):
            grad=problem.partialgrad(x,I)
# getPartialGradient.m:50
        else:
            if 3 == nargin(problem.partialgrad):
                # Obtain, pass along, and save the store for x.
                store=storedb.getWithShared(key)
# getPartialGradient.m:53
                grad,store=problem.partialgrad(x,I,store,nargout=2)
# getPartialGradient.m:54
                storedb.setWithShared(store,key)
            else:
                if 4 == nargin(problem.partialgrad):
                    # Pass along the whole storedb (by reference), with key.
                    grad=problem.partialgrad(x,I,storedb,key)
# getPartialGradient.m:58
                else:
                    up=MException('manopt:getPartialGradient:badpartialgrad','partialgrad should accept 2, 3 or 4 inputs.')
# getPartialGradient.m:60
                    throw(up)
    else:
        if canGetPartialEuclideanGradient(problem):
            ## Compute the partial gradient using the Euclidean partial gradient.
            egrad=getPartialEuclideanGradient(problem,x,I,storedb,key)
# getPartialGradient.m:68
            grad=problem.M.egrad2rgrad(x,egrad)
# getPartialGradient.m:69
        else:
            ## Abandon computing the partial gradient.
            up=MException('manopt:getPartialGradient:fail',cat('The problem description is not explicit enough to ','compute the partial gradient of the cost.'))
# getPartialGradient.m:74
            throw(up)
    
    
    return grad
    
if __name__ == '__main__':
    pass
    