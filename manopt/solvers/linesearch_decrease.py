# Autogenerated with SMOP 
from smop.core import *
# linesearch/linesearch_decrease.m

    
@function
def linesearch_decrease(problem=None,x=None,d=None,f0=None,__=None,options=None,storedb=None,key=None,*args,**kwargs):
    varargin = linesearch_decrease.varargin
    nargin = linesearch_decrease.nargin

    # Backtracking line-search aiming merely for a decrease in cost value.
    
    # function [stepsize, newx, newkey, lsstats] = 
#        linesearch_decrease(problem, x, d, f0, df0, options, storedb, key)
    
    # Line-search algorithm based on a simple backtracking method. The search
# direction provided has to be a descent direction, but needs not be a
# first-order descent, i.e.: this line-search can be used even if x is a
# critical point, as long as the cost function is strictly decreasing
# along the direction d.
    
    # The line-search merely guarantees a decrease in the cost (unless a
# stopping criterion triggers first, such as exceeding a maximal number of
# iterations). This is typically useful to escape saddle points (critical
# points admitting descent directions at the second order). Escape
# directions can be computed using hessianextreme, for example.
# 
# Below, the step is constructed as alpha*d, and the step size is the norm
# of that vector, thus: stepsize = alpha*norm_d. The step is executed by
# retracting the vector alpha*d from the current point x, giving newx.
# An initial stepsize of norm_d thus means the first candidate x is
# obtained by retracting d at x, as is.
    
    # Options:
#   options.ls_max_steps (25): maximum number of cost evaluations.
#   options.ls_initial_stepsize (norm_d): first stepsize trial.
#   options.ls_contraction_factor (0.5): stepsize reduction per iteration.
    
    
    # Inputs/Outputs : see help for linesearch.
#   f0 is the cost at x.
#   df0 is unused.
#   options, storedb and key are optional.
#   Thus, a simplified calling pattern is (with all outputs still
#   available): linesearch_decrease(problem, x, d, f0)
    
    # See also: steepestdescent linesearch hessianextreme
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, April 8, 2015.
# Contributors: 
# Change log:
    
    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# linesearch/linesearch_decrease.m:50
        key=storedb.getNewKey()
# linesearch/linesearch_decrease.m:52
    
    
    norm_d=problem.M.norm(x,d)
# linesearch/linesearch_decrease.m:55
    
    # options structure which is passed to the solver.
    default_options.ls_contraction_factor = copy(0.5)
# linesearch/linesearch_decrease.m:59
    default_options.ls_initial_stepsize = copy(norm_d)
# linesearch/linesearch_decrease.m:60
    default_options.ls_max_steps = copy(25)
# linesearch/linesearch_decrease.m:61
    if logical_not(exist('options','var')) or isempty(options):
        options=struct()
# linesearch/linesearch_decrease.m:64
    
    options=mergeOptions(default_options,options)
# linesearch/linesearch_decrease.m:66
    contraction_factor=options.ls_contraction_factor
# linesearch/linesearch_decrease.m:68
    initial_stepsize=options.ls_initial_stepsize
# linesearch/linesearch_decrease.m:69
    max_ls_steps=options.ls_max_steps
# linesearch/linesearch_decrease.m:70
    
    alpha=initial_stepsize / norm_d
# linesearch/linesearch_decrease.m:73
    
    newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch_decrease.m:76
    newkey=storedb.getNewKey()
# linesearch/linesearch_decrease.m:77
    newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch_decrease.m:78
    cost_evaluations=1
# linesearch/linesearch_decrease.m:79
    
    while newf >= f0:

        # Reduce the step size,
        alpha=dot(contraction_factor,alpha)
# linesearch/linesearch_decrease.m:85
        newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch_decrease.m:88
        newkey=storedb.getNewKey()
# linesearch/linesearch_decrease.m:89
        newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch_decrease.m:90
        cost_evaluations=cost_evaluations + 1
# linesearch/linesearch_decrease.m:91
        if cost_evaluations >= max_ls_steps:
            break

    
    
    # If we got here without obtaining a decrease, we reject the step.
    # Equal cost is accepted, since if x is critical, it is important to
    # move away from x more than it is important to decrease the cost.
    if newf > f0:
        alpha=0
# linesearch/linesearch_decrease.m:104
        newx=copy(x)
# linesearch/linesearch_decrease.m:105
        newkey=copy(key)
# linesearch/linesearch_decrease.m:106
        newf=copy(f0)
# linesearch/linesearch_decrease.m:107
    
    
    # As seen outside this function, stepsize is the size of the vector we
    # retract to make the step from x to newx. Since the step is alpha*d:
    stepsize=dot(alpha,norm_d)
# linesearch/linesearch_decrease.m:112
    
    lsstats.costevals = copy(cost_evaluations)
# linesearch/linesearch_decrease.m:115
    lsstats.stepsize = copy(stepsize)
# linesearch/linesearch_decrease.m:116
    lsstats.alpha = copy(alpha)
# linesearch/linesearch_decrease.m:117
    return stepsize,newx,newkey,lsstats
    
if __name__ == '__main__':
    pass
    