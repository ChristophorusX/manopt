# Autogenerated with SMOP 
from smop.core import *
# linesearch/linesearch.m

    
@function
def linesearch(problem=None,x=None,d=None,f0=None,df0=None,options=None,storedb=None,key=None,*args,**kwargs):
    varargin = linesearch.varargin
    nargin = linesearch.nargin

    # Standard line-search algorithm (step size selection) for descent methods.
    
    # function [stepsize, newx, newkey, lsstats] = 
#                 linesearch(problem, x, d, f0, df0, options, storedb, key)
    
    # Base line-search algorithm for descent methods, based on a simple
# backtracking method. The search direction provided has to be a descent
# direction, as indicated by a negative df0 = directional derivative of f
# at x along d.
    
    # The algorithm is invariant under positive scaling of the cost function
# and under offsetting, that is: if the cost function f is replaced by
# 8*f+3 for example, the returned step size will be the same. Furthermore,
# the returned step size is independent of the norm of the search direction
# vector d: only the direction of d is important.
# 
# Below, the step is constructed as alpha*d, and the step size is the norm
# of that vector, thus: stepsize = alpha*norm_d. The step is executed by
# retracting the vector alpha*d from the current point x, giving newx.
    
    # This line-search may create and maintain a structure called lsmem inside
# storedb.internal. This gives the linesearch the opportunity to remember
# what happened in the previous calls. This is typically used to make a
# first guess at the step size, based on previous events.
    
    # Inputs
    
    #  problem : structure holding the description of the optimization problem
#  x : current point on the manifold problem.M
#  d : tangent vector at x (descent direction) -- its norm is irrelevant
#  f0 : cost value at x
#  df0 : directional derivative at x along d
#  options : options structure (see in code for usage)
#  storedb : StoreDB object (handle class: passed by reference) for caching
#  key : key associated to point x in storedb
    
    #  options, storedb and key are optional.
    
    # Outputs
    
    #  stepsize : norm of the vector retracted to reach newx from x.
#  newx : next iterate suggested by the line-search algorithm, such that
#         the retraction at x of the vector alpha*d reaches newx.
#  newkey : key associated to newx in storedb
#  lsstats : statistics about the line-search procedure
#            (stepsize, number of trials etc).
    
    # See also: steepestdescent conjugategradients linesearch_adaptive
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, Dec. 30, 2012.
# Contributors: 
# Change log:
    
    #	Sept. 13, 2013 (NB):
#       User control over the parameters of the linesearch via the options
#       ls_contraction_factor, ls_optimism, ls_suff_decr and ls_max_steps.
#       See in code for the effect of those.
# 
#   Sept. 13, 2013 (NB):
#       The automatic direction reversal feature was removed (it triggered
#       when df0 > 0). Direction reversal is a decision that needs to be
#       made by the solver, so it can know about it.
# 
#	Sept. 13, 2013 (NB):
#       The linesearch is now invariant under rescaling of the cost
#       function f. That is, if f is replaced by 8*f (and hence the
#       directional derivatives of f are scaled accordingly), the
#       stepsizes computed will not change.
# 
#   Nov. 7, 2013 (NB):
#       The linesearch is now invariant under rescaling of the search
#       direction d. The meaning of stepsize is also more clear in the
#       comments. Added a parameter ls_initial_stepsize to give users
#       control over the first step size trial.
    
    #   April 3, 2015 (NB):
#       Works with the new StoreDB class system.
    
    #   April 8, 2015 (NB):
#       Got rid of lsmem input/output: now maintained in storedb.internal.
    
    #   Oct. 7, 2016 (NB):
#       Thanks to Wen Huang, a bug was corrected in the logic around
#       lsmem handling. Specifically, lsmem = storedb.internal.lsmem;
#       was erroneously coded as lsmem = storedb.internal;
    
    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# linesearch/linesearch.m:94
        key=storedb.getNewKey()
# linesearch/linesearch.m:96
    
    # Backtracking default parameters. These can be overwritten in the
    # options structure which is passed to the solver.
    default_options.ls_contraction_factor = copy(0.5)
# linesearch/linesearch.m:101
    default_options.ls_optimism = copy(1 / 0.5)
# linesearch/linesearch.m:102
    default_options.ls_suff_decr = copy(0.0001)
# linesearch/linesearch.m:103
    default_options.ls_max_steps = copy(25)
# linesearch/linesearch.m:104
    default_options.ls_initial_stepsize = copy(1)
# linesearch/linesearch.m:105
    if logical_not(exist('options','var')) or isempty(options):
        options=struct()
# linesearch/linesearch.m:108
    
    options=mergeOptions(default_options,options)
# linesearch/linesearch.m:110
    contraction_factor=options.ls_contraction_factor
# linesearch/linesearch.m:112
    optimism=options.ls_optimism
# linesearch/linesearch.m:113
    suff_decr=options.ls_suff_decr
# linesearch/linesearch.m:114
    max_ls_steps=options.ls_max_steps
# linesearch/linesearch.m:115
    initial_stepsize=options.ls_initial_stepsize
# linesearch/linesearch.m:116
    
    # This is useful to make the linesearch algorithm invariant under the
    # scaling of d. The rationale is that the important information is the
    # search direction, not the size of that vector. The question of how
    # far we should go is precisely what the linesearch algorithm is
    # supposed to answer: the calling algorithm should not need to care.
    norm_d=problem.M.norm(x,d)
# linesearch/linesearch.m:124
    
    alpha=copy(NaN)
# linesearch/linesearch.m:127
    
    # that to compute an initial guess for the step size, as inspired from
    # Nocedal&Wright, p59.
    if isfield(storedb.internal,'lsmem'):
        lsmem=storedb.internal.lsmem
# linesearch/linesearch.m:133
        if isfield(lsmem,'f0'):
            # Pick initial step size based on where we were last time,
            alpha=dot(2,(f0 - lsmem.f0)) / df0
# linesearch/linesearch.m:136
            alpha=dot(optimism,alpha)
# linesearch/linesearch.m:138
    
    
    # If we have no information about the previous iteration (maybe this is
    # the first one?) or if the above formula gave a too small step size
    # (perhaps it is even negative), then fall back to a user supplied
    # suggestion for the first step size (the "a priori").
    # At any rate, the choice should be invariant under rescaling of the
    # cost function f and of the search direction d, and it should be
    # bounded away from zero for convergence guarantees. We must allow it
    # to be close to zero though, for fine convergence.
    if isnan(alpha) or dot(alpha,norm_d) <= eps:
        alpha=initial_stepsize / norm_d
# linesearch/linesearch.m:151
    
    
    # Make the chosen step and compute the cost there.
    newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch.m:156
    newkey=storedb.getNewKey()
# linesearch/linesearch.m:157
    newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch.m:158
    cost_evaluations=1
# linesearch/linesearch.m:159
    
    while newf > f0 + dot(dot(suff_decr,alpha),df0):

        # Reduce the step size,
        alpha=dot(contraction_factor,alpha)
# linesearch/linesearch.m:165
        newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch.m:168
        newkey=storedb.getNewKey()
# linesearch/linesearch.m:169
        newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch.m:170
        cost_evaluations=cost_evaluations + 1
# linesearch/linesearch.m:171
        if cost_evaluations >= max_ls_steps:
            break

    
    
    # If we got here without obtaining a decrease, we reject the step.
    if newf > f0:
        alpha=0
# linesearch/linesearch.m:182
        newx=copy(x)
# linesearch/linesearch.m:183
        newkey=copy(key)
# linesearch/linesearch.m:184
        newf=copy(f0)
# linesearch/linesearch.m:185
    
    
    # As seen outside this function, stepsize is the size of the vector we
    # retract to make the step from x to newx. Since the step is alpha*d:
    stepsize=dot(alpha,norm_d)
# linesearch/linesearch.m:190
    
    # we will know something about the previous decision.
    storedb.internal.lsmem.f0 = copy(f0)
# linesearch/linesearch.m:194
    storedb.internal.lsmem.df0 = copy(df0)
# linesearch/linesearch.m:195
    storedb.internal.lsmem.stepsize = copy(stepsize)
# linesearch/linesearch.m:196
    
    lsstats.costevals = copy(cost_evaluations)
# linesearch/linesearch.m:199
    lsstats.stepsize = copy(stepsize)
# linesearch/linesearch.m:200
    lsstats.alpha = copy(alpha)
# linesearch/linesearch.m:201
    return stepsize,newx,newkey,lsstats
    
if __name__ == '__main__':
    pass
    