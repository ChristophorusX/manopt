# Autogenerated with SMOP 
from smop.core import *
# linesearch/linesearch_adaptive.m

    
@function
def linesearch_adaptive(problem=None,x=None,d=None,f0=None,df0=None,options=None,storedb=None,key=None,*args,**kwargs):
    varargin = linesearch_adaptive.varargin
    nargin = linesearch_adaptive.nargin

    # Adaptive line search algorithm (step size selection) for descent methods.
    
    # function [stepsize, newx, newkey, lsstats] = 
#        linesearch_adaptive(problem, x, d, f0, df0, options, storedb, key)
    
    # Adaptive linesearch algorithm for descent methods, based on a simple
# backtracking method. Contrary to linesearch.m, this function is not
# invariant under rescaling of the search direction d. These two line
# search methods vary mainly in their strategy to pick the initial step
# size.
# 
# Below, the step is constructed as alpha*d, and the step size is the norm
# of that vector, thus: stepsize = alpha*norm_d. The step is executed by
# retracting the vector alpha*d from the current point x, giving newx.
    
    # This line-search may create and maintain a structure called lsmem inside
# storedb.internal. This gives the linesearch the opportunity to remember
# what happened in the previous calls. This is typically used to make a
# first guess at the step size, based on previous events.
    
    # Inputs/Outputs : see help for linesearch
    
    # See also: steepestdescent conjugategradients linesearch
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Bamdev Mishra, Dec. 30, 2012.
# Contributors: Nicolas Boumal
# Change log:
    
    #   Sept. 13, 2013 (NB) :
#       The automatic direction reversal feature was removed (it triggered
#       when df0 > 0). Direction reversal is a decision that needs to be
#       made by the solver, so it can know about it.
    
    #	Nov. 7, 2013 (NB) :
#       The whole function has been recoded to mimick more closely the new
#       version of linesearch.m. The parameters are available through the
#       options structure passed to the solver and have the same names and
#       same meaning as for the base linesearch. The information is logged
#       more reliably.
    
    #   April 3, 2015 (NB):
#       Works with the new StoreDB class system.
    
    #   April 8, 2015 (NB):
#       Got rid of lsmem input/output: now maintained in storedb.internal.
    
    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# linesearch/linesearch_adaptive.m:54
        key=storedb.getNewKey()
# linesearch/linesearch_adaptive.m:56
    
    # Backtracking default parameters. These can be overwritten in the
    # options structure which is passed to the solver.
    default_options.ls_contraction_factor = copy(0.5)
# linesearch/linesearch_adaptive.m:61
    default_options.ls_suff_decr = copy(0.5)
# linesearch/linesearch_adaptive.m:62
    default_options.ls_max_steps = copy(10)
# linesearch/linesearch_adaptive.m:63
    default_options.ls_initial_stepsize = copy(1)
# linesearch/linesearch_adaptive.m:64
    if logical_not(exist('options','var')) or isempty(options):
        options=struct()
# linesearch/linesearch_adaptive.m:67
    
    options=mergeOptions(default_options,options)
# linesearch/linesearch_adaptive.m:69
    contraction_factor=options.ls_contraction_factor
# linesearch/linesearch_adaptive.m:71
    suff_decr=options.ls_suff_decr
# linesearch/linesearch_adaptive.m:72
    max_ls_steps=options.ls_max_steps
# linesearch/linesearch_adaptive.m:73
    initial_stepsize=options.ls_initial_stepsize
# linesearch/linesearch_adaptive.m:74
    
    norm_d=problem.M.norm(x,d)
# linesearch/linesearch_adaptive.m:77
    
    # filled with a suggestion for the initial step.
    if isfield(storedb.internal,'lsmem'):
        lsmem=storedb.internal.lsmem
# linesearch/linesearch_adaptive.m:82
        if isfield(lsmem,'init_alpha'):
            # Pick initial step size based on where we were last time,
            alpha=lsmem.init_alpha
# linesearch/linesearch_adaptive.m:85
        # Otherwise, fall back to a user supplied suggestion.
    else:
        alpha=initial_stepsize / norm_d
# linesearch/linesearch_adaptive.m:89
    
    # Make the chosen step and compute the cost there.
    newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch_adaptive.m:93
    newkey=storedb.getNewKey()
# linesearch/linesearch_adaptive.m:94
    newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch_adaptive.m:95
    cost_evaluations=1
# linesearch/linesearch_adaptive.m:96
    
    while newf > f0 + dot(dot(suff_decr,alpha),df0):

        # Reduce the step size,
        alpha=dot(contraction_factor,alpha)
# linesearch/linesearch_adaptive.m:102
        newx=problem.M.retr(x,d,alpha)
# linesearch/linesearch_adaptive.m:105
        newkey=storedb.getNewKey()
# linesearch/linesearch_adaptive.m:106
        newf=getCost(problem,newx,storedb,newkey)
# linesearch/linesearch_adaptive.m:107
        cost_evaluations=cost_evaluations + 1
# linesearch/linesearch_adaptive.m:108
        if cost_evaluations >= max_ls_steps:
            break

    
    
    # If we got here without obtaining a decrease, we reject the step.
    if newf > f0:
        alpha=0
# linesearch/linesearch_adaptive.m:119
        newx=copy(x)
# linesearch/linesearch_adaptive.m:120
        newkey=copy(key)
# linesearch/linesearch_adaptive.m:121
        newf=copy(f0)
# linesearch/linesearch_adaptive.m:122
    
    
    # As seen outside this function, stepsize is the size of the vector we
    # retract to make the step from x to newx. Since the step is alpha*d:
    stepsize=dot(alpha,norm_d)
# linesearch/linesearch_adaptive.m:127
    
    # trial should be. On average we intend to do only one extra cost
    # evaluation. Notice how the suggestion is not about stepsize but about
    # alpha. This is the reason why this line search is not invariant under
    # rescaling of the search direction d.
    if 1 == cost_evaluations:
        # If things go very well, push your luck.
        init_alpha=dot(2,alpha)
# linesearch/linesearch_adaptive.m:137
    else:
        if 2 == cost_evaluations:
            # If things go reasonably well, try to keep pace.
            init_alpha=copy(alpha)
# linesearch/linesearch_adaptive.m:140
        else:
            # If we backtracked a lot, the new stepsize is probably quite
            # small: try to recover.
            init_alpha=dot(2,alpha)
# linesearch/linesearch_adaptive.m:144
    
    storedb.internal.lsmem.init_alpha = copy(init_alpha)
# linesearch/linesearch_adaptive.m:146
    
    lsstats.costevals = copy(cost_evaluations)
# linesearch/linesearch_adaptive.m:149
    lsstats.stepsize = copy(stepsize)
# linesearch/linesearch_adaptive.m:150
    lsstats.alpha = copy(alpha)
# linesearch/linesearch_adaptive.m:151
    return stepsize,newx,newkey,lsstats
    
if __name__ == '__main__':
    pass
    