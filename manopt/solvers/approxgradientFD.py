# Autogenerated with SMOP 
from smop.core import *
# gradientapproximations/approxgradientFD.m

    
@function
def approxgradientFD(problem=None,options=None,*args,**kwargs):
    varargin = approxgradientFD.varargin
    nargin = approxgradientFD.nargin

    # Gradient approx. fnctn handle based on finite differences of the cost.
    
    # function gradfun = approxgradientFD(problem)
# function gradfun = approxgradientFD(problem, options)
    
    # Input:
    
    # A Manopt problem structure (already containing the manifold and enough
# information to compute the cost) and an options structure (optional),
# containing one option:
#    options.stepsize (positive double; default: 2^-23).
#    options.subspacedim (positive integer; default: [], for M.dim()).
    
    # If the cost cannot be computed on 'problem', a warning is issued.
    
    # Output:
# 
# Returns a function handle, encapsulating a generic finite difference
# approximation of the gradient of the problem cost. The finite difference
# is based on M.dim()+1 computations of the cost.
# 
# The returned gradfun has this calling pattern:
# 
#   function gradfd = gradfun(x)
#   function gradfd = gradfun(x, storedb)
#   function gradfd = gradfun(x, storedb, key)
# 
# x is a point on the manifold problem.M, storedb is a StoreDB object,
# and key is the StoreDB key to point x.
    
    # Usage:
    
    # Typically, the user will set problem.M and other fields to define the
# cost (typically, problem.cost). Then, to use this generic purpose
# gradient approximation:
    
    #   problem.approxgrad = approxgradientFD(problem, options);
    
    # See also: steepestdescent conjugategradient
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, Nov. 1, 2016.
# Contributors: 
# Change log:
    
    # This gradient approximation is based on the cost:
    # check availability.
    if logical_not(canGetCost(problem)):
        warning('manopt:approxgradFD:nocost','approxgradFD requires the cost to be computable.')
    
    # Set local defaults here, and merge with user options, if any.
    localdefaults.stepsize = copy(2 ** - 23)
# gradientapproximations/approxgradientFD.m:55
    localdefaults.subspacedim = copy([])
# gradientapproximations/approxgradientFD.m:56
    if logical_not(exist('options','var')) or isempty(options):
        options=struct()
# gradientapproximations/approxgradientFD.m:58
    
    options=mergeOptions(localdefaults,options)
# gradientapproximations/approxgradientFD.m:60
    
    # How far do we look?
    stepsize=options.stepsize
# gradientapproximations/approxgradientFD.m:64
    
    # what dimension? If [], uses full tangent space.
    subspacedim=options.subspacedim
# gradientapproximations/approxgradientFD.m:67
    
    # funhandle makes it possible to make storedb and key optional.
    gradfun=funhandle
# gradientapproximations/approxgradientFD.m:71
    
@function
def funhandle(x=None,storedb=None,key=None,*args,**kwargs):
    varargin = funhandle.varargin
    nargin = funhandle.nargin

    # Allow omission of the key, and even of storedb.
    if logical_not(exist('key','var')):
        if logical_not(exist('storedb','var')):
            storedb=StoreDB()
# gradientapproximations/approxgradientFD.m:76
        key=storedb.getNewKey()
# gradientapproximations/approxgradientFD.m:78
    
    gradfd=gradientFD(stepsize,subspacedim,problem,x,storedb,key)
# gradientapproximations/approxgradientFD.m:80
    return gradfd
    
if __name__ == '__main__':
    pass
    
    
    return gradfd
    
if __name__ == '__main__':
    pass
    
    
@function
def gradientFD(stepsize=None,subspacedim=None,problem=None,x=None,storedb=None,key=None,*args,**kwargs):
    varargin = gradientFD.varargin
    nargin = gradientFD.nargin

    # This function does the actual work.
    
    # Original code: Nov. 1, 2016 (NB).
    
    # Evaluate the cost at the root point
    fx=getCost(problem,x,storedb,key)
# gradientapproximations/approxgradientFD.m:92
    
    # thereof. The default is a full subspace. If a strict subspace is
    # picked, the returned vector approximates the orthogonal projection of
    # the gradient to that subspace.
    B=tangentorthobasis(problem.M,x,subspacedim)
# gradientapproximations/approxgradientFD.m:98
    
    # along each direction in the basis B.
    df=zeros(size(B))
# gradientapproximations/approxgradientFD.m:102
    for k in arange(1,numel(B)).reshape(-1):
        # Move in the B{k} direction
        xk=problem.M.retr(x,B[k],stepsize)
# gradientapproximations/approxgradientFD.m:105
        fxk=getCost(problem,xk,storedb)
# gradientapproximations/approxgradientFD.m:107
        df[k]=(fxk - fx) / stepsize
# gradientapproximations/approxgradientFD.m:109
    
    
    # Build the gradient approximation.
    gradfd=lincomb(problem.M,x,B,df)
# gradientapproximations/approxgradientFD.m:113
    return gradfd
    
if __name__ == '__main__':
    pass
    