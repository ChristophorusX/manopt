# Autogenerated with SMOP 
from smop.core import *
# checkgradient.m

    
@function
def checkgradient(problem=None,x=None,d=None,*args,**kwargs):
    varargin = checkgradient.varargin
    nargin = checkgradient.nargin

    # Checks the consistency of the cost function and the gradient.
    
    # function checkgradient(problem)
# function checkgradient(problem, x)
# function checkgradient(problem, x, d)
    
    # checkgradient performs a numerical test to check that the gradient
# defined in the problem structure agrees up to first order with the cost
# function at some point x, along some direction d. The test is based on a
# truncated Taylor series (see online Manopt documentation).
    
    # It is also tested that the gradient is indeed a tangent vector.
# 
# Both x and d are optional and will be sampled at random if omitted.
    
    # See also: checkdiff checkhessian
    
    # This file is part of Manopt: www.manopt.org.
# Original author: Nicolas Boumal, Dec. 30, 2012.
# Contributors: 
# Change log:
    
    #   April 3, 2015 (NB):
#       Works with the new StoreDB class system.
    
    #   Nov. 1, 2016 (NB):
#       Now calls checkdiff with force_gradient = true, instead of doing an
#       rmfield of problem.diff. This became necessary after getGradient
#       was updated to know how to compute the gradient from directional
#       derivatives.
    
    
    # Verify that the problem description is sufficient.
    if logical_not(canGetCost(problem)):
        error('It seems no cost was provided.')
    
    if logical_not(canGetGradient(problem)):
        warning('manopt:checkgradient:nograd','It seems no gradient was provided.')
    
    
    x_isprovided=exist('x','var') and logical_not(isempty(x))
# checkgradient.m:43
    d_isprovided=exist('d','var') and logical_not(isempty(d))
# checkgradient.m:44
    if logical_not(x_isprovided) and d_isprovided:
        error('If d is provided, x must be too, since d is tangent at x.')
    
    
    # If x and / or d are not specified, pick them at random.
    if logical_not(x_isprovided):
        x=problem.M.rand()
# checkgradient.m:52
    
    if logical_not(d_isprovided):
        d=problem.M.randvec(x)
# checkgradient.m:55
    
    ## Check that the gradient yields a first order model of the cost.
    
    # Call checkdiff with force_gradient set to true, to force that
    # function to make a gradient call.
    checkdiff(problem,x,d,true)
    title(sprintf(cat('Gradient check.\\nThe slope of the continuous line ','should match that of the dashed\\n(reference) line ','over at least a few orders of magnitude for h.')))
    xlabel('h')
    ylabel('Approximation error')
    
    if isfield(problem.M,'tangent'):
        storedb=StoreDB()
# checkgradient.m:71
        key=storedb.getNewKey()
# checkgradient.m:72
        grad=getGradient(problem,x,storedb,key)
# checkgradient.m:73
        pgrad=problem.M.tangent(x,grad)
# checkgradient.m:74
        residual=problem.M.lincomb(x,1,grad,- 1,pgrad)
# checkgradient.m:75
        err=problem.M.norm(x,residual)
# checkgradient.m:76
        fprintf('The residual should be 0, or very close. Residual: %g.\\n',err)
        fprintf('If it is far from 0, then the gradient is not in the tangent space.\\n')
    else:
        fprintf(cat('Unfortunately, Manopt was unable to verify that the ','gradient is indeed a tangent vector.\\nPlease verify ','this manually or implement the \'tangent\' function ','in your manifold structure.'))
    
    return
    
if __name__ == '__main__':
    pass
    